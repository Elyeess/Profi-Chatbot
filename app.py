import streamlit as st
import faiss
import numpy as np
import json
import openai
from langchain.embeddings.openai import OpenAIEmbeddings

# Configuration de l'API OpenAI directement dans le code (Non recommand√© mais selon votre demande)
openai.api_key = "saisir votre cl√©"

if not openai.api_key:
    st.error("Cl√© API OpenAI non d√©finie.")
    st.stop()

# Chargement s√©curis√© des fichiers FAISS et m√©tadonn√©es
def load_files(index_path, metadata_path):
    try:
        index = faiss.read_index(index_path)
        with open(metadata_path, "r", encoding="utf-8") as meta_file:
            metadata = json.load(meta_file)
        return index, metadata
    except FileNotFoundError as e:
        st.error(f"‚ùå Fichier non trouv√© : {e}")
    except Exception as e:
        st.error(f"‚ùå Erreur lors du chargement des fichiers : {e}")
    st.stop()

index, metadata = load_files("embeddings_index.faiss", "metadata.json")

# G√©n√©ration des embeddings via LangChain avec OpenAI
def get_embedding(query):
    try:
        embeddings = OpenAIEmbeddings(model="text-embedding-3-large", openai_api_key=openai.api_key)
        embedding = embeddings.embed_query(query)
        return np.array(embedding, dtype=np.float32)
    except Exception as e:
        st.error(f"‚ùå Erreur lors de la g√©n√©ration des embeddings : {e}")
        return None

# Recherche FAISS optimis√©e
def search_faiss(query, k=3):
    embedding = get_embedding(query)
    if embedding is None:
        return []

    distances, indices = index.search(embedding.reshape(1, -1), k)
    results = []
    for i, idx in enumerate(indices[0]):
        if idx < len(metadata):
            result = metadata[idx]
            similarity = 1 / (1 + distances[0][i])
            result["relevance"] = round(similarity * 100, 2)
            results.append(result)
    return results

# Mod√®le d'explications
def explanation_model(query, context):
    try:
        prompt = f"""
        Vous √™tes un assistant p√©dagogique sp√©cialis√© en math√©matiques et sciences.
        Voici une question √† expliquer en d√©tail :

        ### Question :
        {query}

        ### Contexte :
        {context}

        Fournissez une explication claire et d√©taill√©e en utilisant des termes simples et des notations math√©matiques en LaTeX si n√©cessaire.
        """
        response = openai.ChatCompletion.create(
            model="gpt-4-turbo",
            messages=[{"role": "user", "content": prompt}]
        )
        return response["choices"][0]["message"]["content"]
    except Exception as e:
        st.error(f"‚ùå Erreur lors de l'interrogation du mod√®le d'explication : {e}")
        return "Erreur lors de l'interrogation du mod√®le d'explication."

# Mod√®le pour g√©n√©rer des exemples
def example_model(explanation):
    try:
        prompt = f"""
        Vous √™tes un assistant p√©dagogique qui g√©n√®re des exemples pour clarifier des concepts.
        Voici une explication d'un concept math√©matique ou scientifique :

        ### Explication :
        {explanation}

        Fournissez un exemple concret et clair pour illustrer cette explication.
        """
        response = openai.ChatCompletion.create(
            model="gpt-4-turbo",
            messages=[{"role": "user", "content": prompt}]
        )
        return response["choices"][0]["message"]["content"]
    except Exception as e:
        st.error(f"‚ùå Erreur lors de l'interrogation du mod√®le d'exemples : {e}")
        return "Erreur lors de l'interrogation du mod√®le d'exemples."

# Initialisation de l'historique des messages
if "messages" not in st.session_state:
    st.session_state.messages = []
if "user_questions" not in st.session_state:
    st.session_state.user_questions = []

st.title("üíª Profi Chatbot")
st.markdown("Posez une question et recevez des r√©ponses contextuelles bas√©es sur les donn√©es index√©es.")

# Barre lat√©rale pour les questions pos√©es par l'utilisateur
with st.sidebar:
    st.header("üìã Questions pos√©es")
    if st.session_state.user_questions:
        for i, question in enumerate(st.session_state.user_questions, 1):
            st.markdown(f"**{i}.** {question}")
    else:
        st.markdown("Aucune question pos√©e pour le moment.")

for msg in st.session_state.messages:
    role = "üë§ Vous" if msg["role"] == "user" else "üí¨ Profi Chatbot"
    if msg["role"] == "user":
        with st.chat_message(msg["role"]):
            st.write(f"*{role} :* {msg['content']}")
    else:
        with st.chat_message(msg["role"]):
            st.markdown(msg['content'], unsafe_allow_html=True)

# Entr√©e utilisateur
user_input = st.chat_input("Posez votre question ici...")

if user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})
    st.session_state.user_questions.append(user_input)

    with st.spinner("Recherche en cours..."):
        results = search_faiss(user_input, k=3)
        context = "\n\n".join([result["content"] for result in results]) if results else ""

        # Mod√®le d'explications
        explanation = explanation_model(user_input, context)

        # Mod√®le d'exemples
        example = example_model(explanation)

        # Combinaison des deux r√©ponses
        response_content = f"### Explication\n{explanation}\n\n### Exemple\n{example}"

    st.session_state.messages.append({"role": "assistant", "content": response_content})

    with st.chat_message("assistant"):
        st.markdown(response_content, unsafe_allow_html=True)
