{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NubMt8AYhk0x",
        "outputId": "988e75bb-f4cc-450a-996b-e58140a5e07e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapitre: Continuité et limites...\n",
            "Chapitre: Suites réelles...\n",
            "Chapitre: Dérivabilité...\n",
            "Chapitre: Fonctions réciproques...\n",
            "Chapitre: Primitives...\n",
            "Chapitre: Intégrales...\n",
            "Chapitre: Fonction logarithme népérien...\n",
            "Chapitre: Fonction exponentielle...\n",
            "Chapitre: Équations différentielles...\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Charger le fichier JSON\n",
        "with open('last_optimize.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Extraire les chapitres et leurs contenus\n",
        "chapters = [(item[\"chapter\"], item[\"content\"]) for item in data[\"data\"]]\n",
        "\n",
        "# Vérifier les chapitres\n",
        "for chapter, content in chapters:\n",
        "    print(f\"Chapitre: {chapter[:50]}...\")  # Imprimer les 50 premiers caractères du contenu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N84xK2t5h-Cx",
        "outputId": "bf5aeddd-c6b9-41e3-b5ff-4d2a84a67928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.57.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***J'ai juste testé la clé API son fonctionnement***"
      ],
      "metadata": {
        "id": "GIS4_C1BBQl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Remplacez \"votre_cle_api\" par votre clé API OpenAI\n",
        "openai.api_key = \"saisir votre clé\"\n"
      ],
      "metadata": {
        "id": "zWAUhcbxiqkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"Tu es un assistant utile.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Explique-moi comment utiliser l'API GPT-4 sur Colab.\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Afficher la réponse\n",
        "print(response['choices'][0]['message']['content'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9BEp5aNiwN7",
        "outputId": "d2b2f863-61e7-4b04-ebd2-c58fd517ca18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Je suis l'IA GPT-3 et à ma connaissance, au moment de répondre à cette question, OpenAI n'a pas encore sorti la version GPT-4. Par conséquent, il n'existe pas encore d'API GPT-4 que vous pouvez utiliser dans Google Colab.\n",
            "\n",
            "Cependant, je peux vous concevoir une brève idée sur la façon dont vous utiliseriez l'API GPT-3 dans Google Colab, et ce processus serait probablement similaire si et quand l'API GPT-4 serait disponible.\n",
            "\n",
            "1. Connectez-vous à votre compte Google Colab et ouvrez un nouveau carnet de notes.\n",
            "\n",
            "2. Dans la première cellule, tapez le code suivant pour installer la bibliothèque OpenAI : \n",
            "   \n",
            "```python\n",
            "!pip install openai\n",
            "```\n",
            "\n",
            "3. La clé API est nécessaire pour accéder à l'IA de OpenAI. Si vous ne l'avez pas encore, créez-en une sur le site de OpenAI. Ensuite, importez la bibliothèque openai et réglez votre clé API :\n",
            "   \n",
            "```python\n",
            "import openai\n",
            "openai.api_key = 'your-api-key'\n",
            "```\n",
            "   \n",
            "(Substituez 'your-api-key' par votre clé API réelle.)\n",
            "\n",
            "4. Maintenant vous pouvez appeler GPT-3 et lui donner une invite. Par exemple :\n",
            "   \n",
            "```python\n",
            "response = openai.Completion.create(engine=\"text-davinci-003\", prompt=\"Translate the following English text to French: '{}'\", max_tokens=60)\n",
            "print(response.choices[0].text.strip())\n",
            "```\n",
            "   \n",
            "5. Vous pouvez maintenant exécuter la cellule et le texte indiqué sera traduit de l'anglais au français.\n",
            "\n",
            "Veuillez noter qu'il s'agit d'un exemple simple et que le nombre de tokens, l'invite, etc., peuvent être modifiés en fonction de vos besoins. \n",
            "\n",
            "N'oubliez pas également que l'utilisation de l'API coûte de l'argent après un certain nombre de requêtes, donc utilisez-la prudemment. \n",
            "\n",
            "Et pour GPT-4 (si et quand il sera disponible), il y aura vraisemblablement des notes et une documentation de mise à jour de la part de OpenAI concernant les spécificités de son intégration.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhWWDPBsiys9",
        "outputId": "017f865e-1f1f-4285-e072-36a96a67793f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.11.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.12.14)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.28) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bl9BoIbAi3x4",
        "outputId": "8b2d9770-0bef-447e-def3-21916e0d504a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.11.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2024.12.14)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extraire le contenu pour chaque chapitre**"
      ],
      "metadata": {
        "id": "r9HM6dPyBb21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Charger le fichier JSON\n",
        "with open('last_optimize.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Extraire les chapitres et leurs contenus\n",
        "chapters = [(item[\"chapter\"], item[\"content\"]) for item in data[\"data\"]]\n",
        "\n",
        "# Vérifier les chapitres\n",
        "for chapter, content in chapters:\n",
        "    print(f\"Chapitre: {chapter[:50]}...\")  # Afficher les premiers caractères pour vérifier\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRuegf7albbI",
        "outputId": "e67d46ae-1b5f-4eb2-be80-a79276d3790c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapitre: Continuité et limites...\n",
            "Chapitre: Suites réelles...\n",
            "Chapitre: Dérivabilité...\n",
            "Chapitre: Fonctions réciproques...\n",
            "Chapitre: Primitives...\n",
            "Chapitre: Intégrales...\n",
            "Chapitre: Fonction logarithme népérien...\n",
            "Chapitre: Fonction exponentielle...\n",
            "Chapitre: Équations différentielles...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDfvyW8Nlxum",
        "outputId": "07586a91-d007-4aca-e001-4e65eff2bc26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Diviser chaque chapitre par frangment pour pouvoir faire les embeddings**\n",
        "Modele utilisé pour l'embedding: text-embedding-ada-00 fait partie de la bibliothèque Open AI"
      ],
      "metadata": {
        "id": "WhwbhFjEB7f1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "# Fonction pour découper un texte long en fragments\n",
        "def split_text_into_chunks(text, max_tokens=1000):\n",
        "    \"\"\"\n",
        "    Découpe un texte en fragments de longueur maximale `max_tokens`.\n",
        "\n",
        "    Args:\n",
        "        text (str): Texte à découper.\n",
        "        max_tokens (int): Nombre maximal de tokens par fragment.\n",
        "\n",
        "    Returns:\n",
        "        list[str]: Liste des fragments découpés.\n",
        "    \"\"\"\n",
        "    # Initialiser le tokenizer pour le modèle OpenAI\n",
        "    tokenizer = tiktoken.encoding_for_model(\"text-embedding-ada-002\")\n",
        "    tokens = tokenizer.encode(text)\n",
        "\n",
        "    # Découper les tokens en morceaux de taille max_tokens\n",
        "    chunks = [tokens[i:i + max_tokens] for i in range(0, len(tokens), max_tokens)]\n",
        "\n",
        "    # Décoder chaque fragment en texte\n",
        "    return [tokenizer.decode(chunk) for chunk in chunks]\n",
        "\n",
        "# Exemple : découper un chapitre en fragments\n",
        "fragments_by_chapter = {}\n",
        "for chapter, content in chapters:\n",
        "    fragments = split_text_into_chunks(content, max_tokens=1000)\n",
        "    fragments_by_chapter[chapter] = fragments\n",
        "\n",
        "print(\"Fragments générés pour chaque chapitre :\")\n",
        "for chapter, fragments in fragments_by_chapter.items():\n",
        "    print(f\"- {chapter}: {len(fragments)} fragments\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdmuhbMqmrDS",
        "outputId": "813240d9-5b3f-4e8a-92d5-227757e55696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fragments générés pour chaque chapitre :\n",
            "- Continuité et limites: 12 fragments\n",
            "- Suites réelles: 14 fragments\n",
            "- Dérivabilité: 14 fragments\n",
            "- Fonctions réciproques: 12 fragments\n",
            "- Primitives: 7 fragments\n",
            "- Intégrales: 15 fragments\n",
            "- Fonction logarithme népérien: 13 fragments\n",
            "- Fonction exponentielle: 17 fragments\n",
            "- Équations différentielles: 13 fragments\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Configuration de la clé API\n",
        "\n",
        "# Fonction pour générer des embeddings\n",
        "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
        "    \"\"\"\n",
        "    Génère un embedding pour un texte donné.\n",
        "\n",
        "    Args:\n",
        "        text (str): Texte à convertir en embedding.\n",
        "        model (str): Modèle à utiliser, par défaut \"text-embedding-ada-002\".\n",
        "\n",
        "    Returns:\n",
        "        list[float]: Embedding sous forme de liste de flottants.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = openai.Embedding.create(\n",
        "            model=model,\n",
        "            input=text\n",
        "        )\n",
        "        return response[\"data\"][0][\"embedding\"]\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur lors de la génération de l'embedding : {e}\")\n",
        "        return None\n",
        "\n",
        "# Générer les embeddings pour chaque fragment de chaque chapitre\n",
        "embeddings_by_chapter = {}\n",
        "for chapter, fragments in fragments_by_chapter.items():\n",
        "    embeddings = [get_embedding(fragment) for fragment in fragments if fragment]\n",
        "    embeddings_by_chapter[chapter] = embeddings\n",
        "\n",
        "print(\"Embeddings générés pour chaque chapitre.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMHaUGUemzuX",
        "outputId": "a4e3fad5-3879-42fe-da33-9e4c3e345282"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings générés pour chaque chapitre.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCNNGe7-oi9s",
        "outputId": "80ac0847-f559-4094-e9ca-a23484870891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.9.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **tape 2 : Indexer les embeddings dans FAISS**\n",
        "Utilisez FAISS pour stocker ces vecteurs dans une base de données vectorielle.\n",
        "Associez des métadonnées (comme les noms de chapitre ou le contenu original) pour pouvoir les identifier lors de la recherche"
      ],
      "metadata": {
        "id": "acIUm2LUzQYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Initialiser l'index FAISS\n",
        "dimension = 1536  # Taille des embeddings générés par text-embedding-ada-002\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "# Ajouter les embeddings de chaque chapitre\n",
        "metadata = []\n",
        "for chapter, embeddings in embeddings_by_chapter.items():\n",
        "    for embedding in embeddings:\n",
        "        index.add(np.array(embedding, dtype=\"float32\").reshape(1, -1))\n",
        "        metadata.append({\"chapter\": chapter})\n",
        "\n",
        "print(f\"Nombre total d'embeddings ajoutés à l'index : {index.ntotal}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeZ5HpcooXOH",
        "outputId": "96a4320e-1b8d-4aca-faa1-9c4caf11e051"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre total d'embeddings ajoutés à l'index : 117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Combinaison de FAISS et OpenAI**\n",
        "FAISS et OpenAI ont des rôles complémentaires dans votre pipeline :\n",
        "\n",
        "2.1. OpenAI : Générer les embeddings\n",
        "Les modèles OpenAI, comme text-embedding-ada-002, transforment le texte en vecteurs d'embedding.\n",
        "Ces vecteurs sont des représentations numériques qui capturent les relations sémantiques entre les mots ou phrases.\n",
        "Exemple :\n",
        "Texte : \"La continuité est importante en mathématiques.\"\n",
        "Embedding généré (taille 1536) : [0.234, -0.567, ..., 0.123]. \\\\\n",
        "2.2. FAISS : Indexer et rechercher\n",
        "Indexer : FAISS prend ces vecteurs d'embedding et les stocke dans une base de données vectorielle.\n",
        "Rechercher : Lorsque vous avez un vecteur d'une requête utilisateur (généré par OpenAI), FAISS trouve les vecteurs les plus proches dans l'index.\n",
        "Les vecteurs les plus proches correspondent aux textes ou fragments les plus pertinents."
      ],
      "metadata": {
        "id": "ejmBRmpEpmXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vérifier les dimensions et le contenu d'un embedding\n",
        "for chapter, embeddings in embeddings_by_chapter.items():\n",
        "    print(f\"Chapitre : {chapter}\")\n",
        "    for i, embedding in enumerate(embeddings[:2]):  # Affichez seulement 2 exemples\n",
        "        print(f\"  Fragment {i} : Taille de l'embedding : {len(embedding)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KN1Vmh-aow4r",
        "outputId": "bba1ba19-3517-4490-e1b4-2702944dc33e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapitre : Continuité et limites\n",
            "  Fragment 0 : Taille de l'embedding : 1536\n",
            "  Fragment 1 : Taille de l'embedding : 1536\n",
            "Chapitre : Suites réelles\n",
            "  Fragment 0 : Taille de l'embedding : 1536\n",
            "  Fragment 1 : Taille de l'embedding : 1536\n",
            "Chapitre : Dérivabilité\n",
            "  Fragment 0 : Taille de l'embedding : 1536\n",
            "  Fragment 1 : Taille de l'embedding : 1536\n",
            "Chapitre : Fonctions réciproques\n",
            "  Fragment 0 : Taille de l'embedding : 1536\n",
            "  Fragment 1 : Taille de l'embedding : 1536\n",
            "Chapitre : Primitives\n",
            "  Fragment 0 : Taille de l'embedding : 1536\n",
            "  Fragment 1 : Taille de l'embedding : 1536\n",
            "Chapitre : Intégrales\n",
            "  Fragment 0 : Taille de l'embedding : 1536\n",
            "  Fragment 1 : Taille de l'embedding : 1536\n",
            "Chapitre : Fonction logarithme népérien\n",
            "  Fragment 0 : Taille de l'embedding : 1536\n",
            "  Fragment 1 : Taille de l'embedding : 1536\n",
            "Chapitre : Fonction exponentielle\n",
            "  Fragment 0 : Taille de l'embedding : 1536\n",
            "  Fragment 1 : Taille de l'embedding : 1536\n",
            "Chapitre : Équations différentielles\n",
            "  Fragment 0 : Taille de l'embedding : 1536\n",
            "  Fragment 1 : Taille de l'embedding : 1536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Afficher les vecteurs pour chaque fragment\n",
        "for chapter, embeddings in embeddings_by_chapter.items():\n",
        "    print(f\"Chapitre : {chapter}\")\n",
        "    for i, embedding in enumerate(embeddings[:2]):  # Affichez 2 vecteurs par chapitre pour éviter une sortie trop longue\n",
        "        print(f\"  Fragment {i} : Embedding : {embedding[:10]}\")  # Affiche les 10 premières valeurs du vecteur\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UIEOV6d0MZl",
        "outputId": "c9bb136f-f0c4-4cae-ffc1-89447175fef1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapitre : Continuité et limites\n",
            "  Fragment 0 : Embedding : [-0.003102038986980915, -0.010572356171905994, -0.003379389876499772, -0.00848561991006136, -0.012256273068487644, 0.004269224591553211, -0.01389396470040083, -0.002595213009044528, -0.025080455467104912, -0.03069351240992546]\n",
            "  Fragment 1 : Embedding : [0.005111632868647575, -0.00604903744533658, -0.004451856482774019, 0.003413198748603463, -0.019584249705076218, 0.0002798741334117949, -0.020655568689107895, 0.010641341097652912, -0.01895713433623314, -0.019649572670459747]\n",
            "Chapitre : Suites réelles\n",
            "  Fragment 0 : Embedding : [0.007688635960221291, 0.0009916818235069513, 0.008085639216005802, 0.0027459412813186646, -0.012095375917851925, 0.0066994354128837585, -0.03311009705066681, -0.018010728061199188, -0.019426709040999413, -0.029775267466902733]\n",
            "  Fragment 1 : Embedding : [-0.007832339033484459, -0.022344032302498817, 0.006553454324603081, 0.00982024148106575, -0.01135092694312334, 0.021972956135869026, -0.02356327883899212, -0.006059791427105665, -0.024053629487752914, -0.03397989273071289]\n",
            "Chapitre : Dérivabilité\n",
            "  Fragment 0 : Embedding : [0.018378211185336113, 0.005601168610155582, -0.0017906463472172618, -0.023866426199674606, -0.018271902576088905, 0.003385285148397088, -0.022152189165353775, -0.00631875591352582, -0.017541024833917618, -0.0262849610298872]\n",
            "  Fragment 1 : Embedding : [0.006015031132847071, -0.013077025301754475, -0.0013478804612532258, 0.0076288203708827496, -0.016671372577548027, -0.0023973435163497925, -0.01563107967376709, 0.00013420455798041075, 0.002802457893267274, -0.01944549009203911]\n",
            "Chapitre : Fonctions réciproques\n",
            "  Fragment 0 : Embedding : [-0.016141965985298157, -0.0040654088370501995, 0.021247828379273415, 0.018202926963567734, -0.011075994931161404, 0.009566840715706348, -0.019133681431412697, -0.0013080444186925888, -0.040687330067157745, -0.017312059178948402]\n",
            "  Fragment 1 : Embedding : [-0.014866105280816555, -0.00498344749212265, -0.011968360282480717, 0.017723416909575462, -0.023694120347499847, 0.004643131047487259, -0.02265632152557373, 0.008551716804504395, -0.025500155985355377, -0.0192464180290699]\n",
            "Chapitre : Primitives\n",
            "  Fragment 0 : Embedding : [0.013112256303429604, 0.003055315464735031, -0.0030087707564234734, 0.004664425738155842, -0.02938953973352909, -0.007553510833531618, -0.020200321450829506, 0.015386288985610008, -8.643982437206432e-05, -0.03790053725242615]\n",
            "  Fragment 1 : Embedding : [0.008644294925034046, -0.019537298008799553, 0.005274541210383177, 0.0014128825860098004, -0.04259315878152847, 0.008260692469775677, -0.034577179700136185, -0.007592693902552128, -0.00039703736547380686, -0.03179936483502388]\n",
            "Chapitre : Intégrales\n",
            "  Fragment 0 : Embedding : [0.002595973201096058, -0.008714817464351654, 0.003651514882221818, -0.01136686559766531, -0.00901828519999981, -0.0008774191373959184, -0.03132320195436478, -0.010251949541270733, -0.008312392048537731, -0.024039965122938156]\n",
            "  Fragment 1 : Embedding : [-0.010379357263445854, -0.016131386160850525, 0.002035703742876649, -0.007763631176203489, -0.025745948776602745, 0.005793802440166473, -0.015835750848054886, -0.005019367206841707, -0.012410241179168224, -0.012056765146553516]\n",
            "Chapitre : Fonction logarithme népérien\n",
            "  Fragment 0 : Embedding : [0.010465318337082863, 0.018149735406041145, -0.01426722388714552, -0.003775037592276931, -0.01629580371081829, 0.0007896848255768418, -0.02262335643172264, -0.008369567804038525, -0.008141184225678444, -0.01829751394689083]\n",
            "  Fragment 1 : Embedding : [-0.0009012873633764684, 0.012892146594822407, -0.005276476498693228, -0.009396647103130817, -0.012526648119091988, 0.005492453463375568, -0.02767159789800644, 0.0027827764861285686, 0.005093727260828018, -0.027910834178328514]\n",
            "Chapitre : Fonction exponentielle\n",
            "  Fragment 0 : Embedding : [0.030554840341210365, 0.0064455727115273476, -0.000700105563737452, -0.010932164266705513, -0.00893374066799879, -0.010721803642809391, -0.019129645079374313, -0.015369453467428684, -0.0016368664801120758, -0.017578236758708954]\n",
            "  Fragment 1 : Embedding : [-0.000661373371258378, -0.010143307968974113, -0.002610062714666128, -0.007626039907336235, -0.021163947880268097, -0.006070462521165609, -0.017276691272854805, 0.00489618768915534, -0.013848348520696163, -0.02061055414378643]\n",
            "Chapitre : Équations différentielles\n",
            "  Fragment 0 : Embedding : [0.007535662967711687, 0.005827534943819046, 0.013068007305264473, -0.011973478831350803, -0.010620242916047573, -0.009936992079019547, -0.005784417502582073, -2.5912133423844352e-05, -0.00922720693051815, -0.028709813952445984]\n",
            "  Fragment 1 : Embedding : [-0.012348947115242481, 0.007686523254960775, -0.011390559375286102, 0.0016018986934795976, -0.03577115014195442, -0.007259134203195572, -0.008198094554245472, 0.007770705968141556, -0.011649583466351032, -0.027326982468366623]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Sauvegarder les embeddings avec leurs chapitres et fragments\n",
        "with open(\"embeddings_debug.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(embeddings_by_chapter, f)\n",
        "\n",
        "print(\"Embeddings sauvegardés dans embeddings_debug.json.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SneLVuG0SeI",
        "outputId": "73c2a185-24c6-4440-b3eb-ce0f1e2fbbd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings sauvegardés dans embeddings_debug.json.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(\"embeddings_debug.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "AmqG1dcz0Tn3",
        "outputId": "b7c2d221-88a0-4181-f233-ea551ed028a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d804fd60-95f4-41b7-892b-dad6f640567a\", \"embeddings_debug.json\", 4022176)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction pour rechercher les vecteurs similaires dans FAISS\n",
        "def search_similar_fragments(query, k=5):\n",
        "    query_embedding = get_embedding(query)\n",
        "    if query_embedding:\n",
        "        distances, indices = index.search(np.array(query_embedding, dtype=\"float32\").reshape(1, -1), k)\n",
        "        return distances, indices\n",
        "    return [], []\n"
      ],
      "metadata": {
        "id": "8yY9WCs65mF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **La phase de L'execution**"
      ],
      "metadata": {
        "id": "oHgh8IH30ZFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction pour générer une réponse à partir du contexte\n",
        "def generate_response(query, context):\n",
        "    try:\n",
        "        response = openai.Completion.create(\n",
        "            model=\"text-davinci-003\",\n",
        "            prompt=f\"Contexte : {context}\\n\\nQuestion : {query}\\n\\nRéponse :\",\n",
        "            max_tokens=200\n",
        "        )\n",
        "        return response[\"choices\"][0][\"text\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur lors de la génération de la réponse : {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "5-fBFYBv0ebn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction pour générer une réponse à partir du contexte\n",
        "def generate_response(query, context):\n",
        "    try:\n",
        "        response = openai.Completion.create(\n",
        "            model=\"gpt-3.5-turbo\",  # Remplacez par le modèle le plus récent disponible\n",
        "            prompt=f\"Contexte : {context}\\n\\nQuestion : {query}\\n\\nRéponse :\",\n",
        "            max_tokens=200\n",
        "        )\n",
        "        return response[\"choices\"][0][\"text\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur lors de la génération de la réponse : {e}\")\n",
        "        return None\n",
        "\n",
        "# Fonction principale pour répondre à une question de l'utilisateur\n",
        "def chatbot_response(query):\n",
        "    # Rechercher les fragments pertinents\n",
        "    distances, indices = search_similar_fragments(query)\n",
        "\n",
        "    # Récupérer et combiner les fragments pertinents pour créer un contexte\n",
        "    context = \" \".join([\n",
        "        fragments_by_chapter[metadata[idx][\"chapter\"]][i]\n",
        "        for i, idx in enumerate(indices[0])  # Utiliser i pour accéder aux fragments correctement\n",
        "    ])\n",
        "\n",
        "    # Générer une réponse basée sur le contexte extrait\n",
        "    if context:\n",
        "        response = generate_response(query, context)\n",
        "        return response\n",
        "    else:\n",
        "        return \"Désolé, je n'ai pas trouvé de réponse à votre question.\"\n",
        "\n",
        "# Boucle interactive pour saisir une question\n",
        "while True:\n",
        "    user_query = input(\"Posez une question (ou tapez 'exit' pour quitter) : \")\n",
        "    if user_query.lower() == 'exit':\n",
        "        print(\"Fin de la session.\")\n",
        "        break\n",
        "\n",
        "    response = chatbot_response(user_query)\n",
        "    print(\"Réponse générée :\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WResVft05tlY",
        "outputId": "47ac6dd1-304b-41f0-fdfd-9f5b70c72936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Posez une question (ou tapez 'exit' pour quitter) : primitive\n",
            "Erreur lors de la génération de la réponse : This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?\n",
            "Réponse générée : None\n",
            "Posez une question (ou tapez 'exit' pour quitter) : exit\n",
            "Fin de la session.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Configuration de la clé API OpenAI\n",
        "\n",
        "\n",
        "# Fonction pour rechercher les vecteurs similaires dans FAISS\n",
        "def search_similar_fragments(query, k=5):\n",
        "    query_embedding = get_embedding(query)  # Assurez-vous d'avoir votre fonction pour générer les embeddings\n",
        "    if query_embedding:\n",
        "        distances, indices = index.search(np.array(query_embedding, dtype=\"float32\").reshape(1, -1), k)\n",
        "        return distances, indices\n",
        "    return [], []\n",
        "\n",
        "# Fonction pour générer une réponse à partir du contexte\n",
        "def generate_response(query, context):\n",
        "    try:\n",
        "        # Utilisation du modèle de chat gpt-3.5-turbo avec le point de terminaison approprié\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",  # Modèle de chat correct\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"Vous êtes un assistant éducatif.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Contexte : {context}\\n\\nQuestion : {query}\"}\n",
        "            ]\n",
        "        )\n",
        "        return response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur lors de la génération de la réponse : {e}\")\n",
        "        return None\n",
        "\n",
        "# Fonction principale pour répondre à une question de l'utilisateur\n",
        "def chatbot_response(query):\n",
        "    # Rechercher les fragments pertinents\n",
        "    distances, indices = search_similar_fragments(query)\n",
        "\n",
        "    # Récupérer et combiner les fragments pertinents pour créer un contexte\n",
        "    context = \" \".join([\n",
        "        fragments_by_chapter[metadata[idx][\"chapter\"]][i]\n",
        "        for i, idx in enumerate(indices[0])  # Utiliser i pour accéder aux fragments correctement\n",
        "    ])\n",
        "\n",
        "    # Générer une réponse basée sur le contexte extrait\n",
        "    if context:\n",
        "        response = generate_response(query, context)\n",
        "        return response\n",
        "    else:\n",
        "        return \"Désolé, je n'ai pas trouvé de réponse à votre question.\"\n",
        "\n",
        "# Boucle interactive pour saisir une question\n",
        "while True:\n",
        "    user_query = input(\"Posez une question (ou tapez 'exit' pour quitter) : \")\n",
        "    if user_query.lower() == 'exit':\n",
        "        print(\"Fin de la session.\")\n",
        "        break\n",
        "\n",
        "    response = chatbot_response(user_query)\n",
        "    print(\"Réponse générée :\", response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnCIe-LdAmY1",
        "outputId": "9526d5cf-adbe-47bc-cf9f-8ea20b1b5acc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Posez une question (ou tapez 'exit' pour quitter) : primitive\n",
            "Réponse générée : Une primitive d'une fonction \\( f \\) est une fonction \\( F \\) telle que sa dérivée est égale à la fonction \\( f \\). En d'autres termes, si \\( F \\) est une primitive de \\( f \\) sur un intervalle \\( I \\), alors \\( F'(x) = f(x) \\) pour tout \\( x \\) dans \\( I \\).\n",
            "\n",
            "Pour vérifier si une fonction est une primitive d'une autre fonction sur un intervalle donné, il suffit de dériver la première fonction pour voir si on obtient la seconde fonction. Si c'est le cas, alors la première fonction est bien une primitive de la seconde sur l'intervalle considéré.\n",
            "\n",
            "N'hésitez pas à poser d'autres questions si vous avez besoin de plus d'informations sur les primitives des fonctions.\n",
            "Posez une question (ou tapez 'exit' pour quitter) : exit\n",
            "Fin de la session.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CUVGze7wBSg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eIaCEX0slX0M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}